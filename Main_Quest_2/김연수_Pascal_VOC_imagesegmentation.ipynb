{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa9130ee",
   "metadata": {},
   "source": [
    "### *초반 부분에 에러 해결을 못해서, 전반적인 코드 작성은 일단 했지만, 과제에 맞춰 조정하고 실행하지 못했습니다!ㅠㅠ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0500006f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration nateraw___pascal-voc-2012-c68607404d4811ac\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset parquet/nateraw___pascal-voc-2012 to /aiffel/.cache/huggingface/datasets/parquet/nateraw___pascal-voc-2012-c68607404d4811ac/0.0.0/9296ce43568b20d72ff8ff8ecbc821a16b68e9b8b7058805ef11f06e035f911a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f022f6f1e4a46479face9b6187d74ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431ffb2e47db40db8a2b180d5dabbb0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5b9f0aa2e843579d9f8c7d2e67088f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/167M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d123bd0fb9e44f6d802549ca415e888d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /aiffel/.cache/huggingface/datasets/parquet/nateraw___pascal-voc-2012-c68607404d4811ac/0.0.0/9296ce43568b20d72ff8ff8ecbc821a16b68e9b8b7058805ef11f06e035f911a. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"nateraw/pascal-voc-2012\", split = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f044de11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration nateraw___pascal-voc-2012-c68607404d4811ac\n",
      "Reusing dataset parquet (/aiffel/.cache/huggingface/datasets/parquet/nateraw___pascal-voc-2012-c68607404d4811ac/0.0.0/9296ce43568b20d72ff8ff8ecbc821a16b68e9b8b7058805ef11f06e035f911a)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/2694312736.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Read the image data using PIL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mimage_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'dict'"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Load the Pascal VOC 2012 dataset\n",
    "dataset = load_dataset(\"nateraw/pascal-voc-2012\", split='train')\n",
    "\n",
    "# Choose an index of an image in the dataset to work with\n",
    "image_index = 0\n",
    "\n",
    "# Get the image data for the specified index\n",
    "image_data = dataset[image_index]['image']\n",
    "\n",
    "# Read the image data using PIL\n",
    "image_io = io.BytesIO(image_data)\n",
    "img = Image.open(image_io)\n",
    "\n",
    "# Display the image\n",
    "img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "62d0ad88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'JPEGImages/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/3801058154.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m input_img_paths = sorted(\n\u001b[1;32m      5\u001b[0m     [os.path.join(input_dir, fname)\n\u001b[0;32m----> 6\u001b[0;31m      \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m      if fname.endswith(\".jpeg\")])\n\u001b[1;32m      8\u001b[0m target_paths = sorted(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'JPEGImages/'"
     ]
    }
   ],
   "source": [
    "input_dir = \"JPEGImages/\"\n",
    "target_dir = \"SegmentationClass/\"\n",
    "\n",
    "input_img_paths = sorted(\n",
    "    [os.path.join(input_dir, fname)\n",
    "     for fname in os.listdir(input_dir)\n",
    "     if fname.endswith(\".jpeg\")])\n",
    "target_paths = sorted(\n",
    "    [os.path.join(target_dir, fname)\n",
    "     for fname in os.listdir(target_dir)\n",
    "     if fname.endswith(\".png\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f658cc38",
   "metadata": {},
   "source": [
    "### 입력 이미지와 분할 마스크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ab03c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(load_img(input_img_paths[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d2e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_target(target_array):\n",
    "    normalized_array = (target_array.astype(\"uint8\") - 1) * 127\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(normalized_array[:, :, 0])\n",
    "\n",
    "img = img_to_array(load_img(target_paths[9], color_mode=\"grayscale\"))\n",
    "display_target(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bc8dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "img_size = (200, 200)\n",
    "num_imgs = len(input_img_paths)\n",
    "\n",
    "random.Random(1337).shuffle(input_img_paths)\n",
    "random.Random(1337).shuffle(target_paths)\n",
    "\n",
    "def path_to_input_image(path):\n",
    "    return img_to_array(load_img(path, target_size=img_size))\n",
    "\n",
    "def path_to_target(path):\n",
    "    img = img_to_array(\n",
    "        load_img(path, target_size=img_size, color_mode=\"grayscale\"))\n",
    "    img = img.astype(\"uint8\") - 1\n",
    "    return img\n",
    "\n",
    "input_imgs = np.zeros((num_imgs,) + img_size + (3,), dtype=\"float32\")\n",
    "targets = np.zeros((num_imgs,) + img_size + (1,), dtype=\"uint8\")\n",
    "for i in range(num_imgs):\n",
    "    input_imgs[i] = path_to_input_image(input_img_paths[i])\n",
    "    targets[i] = path_to_target(target_paths[i])\n",
    "\n",
    "num_val_samples = 1000\n",
    "train_input_imgs = input_imgs[:-num_val_samples]\n",
    "train_targets = targets[:-num_val_samples]\n",
    "val_input_imgs = input_imgs[-num_val_samples:]\n",
    "val_targets = targets[-num_val_samples:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8986b84f",
   "metadata": {},
   "source": [
    "### data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41091692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import image\n",
    "from tensorflow.keras.preprocessing.image import random_rotation\n",
    "\n",
    "@tf.function() # 빠른 텐서플로 연산을 위해 @tf.function()을 사용합니다. \n",
    "def apply_augmentation(sketch, colored):\n",
    "    \n",
    "    # 두 이미지가 채널 축으로 연결됩니다. (tf.concat). 두 이미지가 각각 3채널인 경우 6채널이 됩니다.\n",
    "    stacked = tf.concat([sketch, colored], axis=-1)\n",
    "    \n",
    "    # [위,아래][왼,우][앞,뒤] 행렬의 rank를 인수만큼 증가시킨다.\n",
    "    _pad = tf.constant([[30,30],[30,30],[0,0]])\n",
    "    \n",
    "    # 50% 확률로 Refection padding 또는 constant padding이 30픽셀의 pad width 만큼적용됩니다.\n",
    "    if tf.random.uniform(()) < .5:\n",
    "        padded = tf.pad(stacked, _pad, \"REFLECT\")\n",
    "    else:\n",
    "        padded = tf.pad(stacked, _pad, \"CONSTANT\", constant_values=1.)\n",
    "        \n",
    "    # (256,256,6) 크기를 가진 이미지를 임의로 잘라냅니다.\n",
    "    out = image.random_crop(padded, size=[256, 256, 6])\n",
    "    \n",
    "    # 50% 확률로 가로로 뒤집습니다.\n",
    "    out = image.random_flip_left_right(out)\n",
    "    # 50% 확률로 세로로 뒤집습니다.\n",
    "    out = image.random_flip_up_down(out)\n",
    "    \n",
    "    \n",
    "    # 50% 확률로 회전시킵니다.\n",
    "    if tf.random.uniform(()) < .5:\n",
    "        degree = tf.random.uniform([], minval=1, maxval=4, dtype=tf.int32)\n",
    "        out = image.rot90(out, k=degree)\n",
    "    \n",
    "    return out[...,:3], out[...,3:]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d16b47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 함수 적용 결과를 시각화 해보기 \n",
    "\n",
    "plt.figure(figsize=(15,13))\n",
    "img_n = 1\n",
    "for i in range(1, 13, 2):\n",
    "    augmented_sketch, augmented_colored = apply_augmentation(sketch, colored)\n",
    "    \n",
    "    plt.subplot(3,4,i)\n",
    "    plt.imshow(denormalize(augmented_sketch)); plt.title(f\"Image {img_n}\")\n",
    "    plt.subplot(3,4,i+1); \n",
    "    plt.imshow(denormalize(augmented_colored)); plt.title(f\"Image {img_n}\")\n",
    "    img_n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6b145e",
   "metadata": {},
   "source": [
    "### U-Net 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59134d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Conv Block \"\"\"\n",
    "class ConvBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_filters):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = Conv2D(n_filters, 3, padding='same')\n",
    "        self.conv2 = Conv2D(n_filters, 3, padding='same')\n",
    "\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.bn2 = BatchNormalization()\n",
    "\n",
    "        self.activation = Activation('relu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6613018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Encoder Block \"\"\"\n",
    "class EncoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_filters):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.conv_blk = ConvBlock(n_filters)\n",
    "        self.pool = MaxPooling2D((2,2))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv_blk(inputs)\n",
    "        p = self.pool(x)\n",
    "        return x, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8575e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Decoder Block \"\"\"\n",
    "class DecoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_filters):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.up = Conv2DTranspose(n_filters, (2,2), strides=2, padding='same')\n",
    "        self.conv_blk = ConvBlock(n_filters)\n",
    "\n",
    "    def call(self, inputs, skip):\n",
    "        x = self.up(inputs)\n",
    "        x = Concatenate()([x, skip])\n",
    "        x = self.conv_blk(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216a3ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" U-Net Model \"\"\"\n",
    "class UNET(tf.keras.Model):\n",
    "    def __init__(self, n_classes):\n",
    "        super(UNET, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.e1 = EncoderBlock(64)\n",
    "        self.e2 = EncoderBlock(128)\n",
    "        self.e3 = EncoderBlock(256)\n",
    "        self.e4 = EncoderBlock(512)\n",
    "\n",
    "        # Bridge\n",
    "        self.b = ConvBlock(1024)\n",
    "\n",
    "        # Decoder\n",
    "        self.d1 = DecoderBlock(512)\n",
    "        self.d2 = DecoderBlock(256)\n",
    "        self.d3 = DecoderBlock(128)\n",
    "        self.d4 = DecoderBlock(64)\n",
    "\n",
    "        # Outputs\n",
    "        if n_classes == 1:\n",
    "            activation = 'sigmoid'\n",
    "        else:\n",
    "            activation = 'softmax'\n",
    "\n",
    "        self.outputs = Conv2D(n_classes, 1, padding='same', activation=activation)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        s1, p1 = self.e1(inputs)\n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)\n",
    "\n",
    "        b = self.b(p4)\n",
    "\n",
    "        d1 = self.d1(b, s4)\n",
    "        d2 = self.d2(d1, s3)\n",
    "        d3 = self.d3(d2, s2)\n",
    "        d4 = self.d4(d3, s1)\n",
    "\n",
    "        outputs = self.outputs(d4)\n",
    "\n",
    "        return outputs\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f434a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"oxford_segmentation.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model.fit(train_input_imgs, train_targets,\n",
    "                    epochs=50,\n",
    "                    callbacks=callbacks,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(val_input_imgs, val_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cf4238",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(history.history[\"loss\"]) + 1)\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff84fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import array_to_img\n",
    "\n",
    "model = keras.models.load_model(\"oxford_segmentation.keras\")\n",
    "\n",
    "i = 4\n",
    "test_image = val_input_imgs[i]\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(array_to_img(test_image))\n",
    "\n",
    "mask = model.predict(np.expand_dims(test_image, 0))[0]\n",
    "\n",
    "def display_mask(pred):\n",
    "    mask = np.argmax(pred, axis=-1)\n",
    "    mask *= 127\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(mask)\n",
    "\n",
    "display_mask(mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
