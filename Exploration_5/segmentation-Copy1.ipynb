{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11fe4442",
   "metadata": {},
   "source": [
    "# Human Segmentation\n",
    "1) 아웃포커싱 효과가 적용된 인물모드 사진과 동물 사진, 배경전환 크로마키사진 제작  \n",
    "2) 인물사진에서 발생한 문제점을 정확히 지적한 사진을 제출  \n",
    "3) semantic segmentation mask의 오류를 보완할 수 있는 좋은 솔루션을 이유와 함께 제시"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f367b26",
   "metadata": {},
   "source": [
    "## 1-1. 인물모드 사진"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c6727",
   "metadata": {},
   "source": [
    "### 필요한 라이브러리 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca03f45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os: Operating System의 줄임말로, 운영체제에서 제공되는 여러 기능을 파이썬에서 사용할 수 있도록 함 (Ex. 디렉토리 경로 이동, 시스템 환경 변수 가져오기 등)\n",
    "# urllib: URL 작업을 위한 여러 모듈을 모은 패키지. (Ex. urllib.request, urllib.parse, ...)\n",
    "# cv2: OpenCV 라이브러리로, 실시간 컴퓨터 비전을 목적으로 한 프로그래밍 라이브러리\n",
    "# numpy(NumPy): 행렬이나 대규모 다차원 배열을 쉽게 처리할 수 있도록 지원하는 라이브러리. 데이터 구조 외에도 수치 계산을 위해 효율적으로 구현된 기능을 제공\n",
    "# pixellib: 이미지 및 비디오 segmentation을 수행하기 위한 라이브러리. \n",
    "# pixellib.semantic: segmentation 기법 중, semantic segmentation을 쉽게 사용할 수 있도록 만든 라이브러리\n",
    "# matplotlib: 파이썬 프로그래밍 언어 및 수학적 확장 NumPy 라이브러리를 활용한 플로팅 라이브러리로, 데이터 시각화 도구\n",
    "import os\n",
    "import urllib\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pixellib.semantic import semantic_segmentation\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856d7c55",
   "metadata": {},
   "source": [
    "### 사진 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533eae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os 모듈에 있는 getenv() 함수를 이용하여 읽고싶은 파일의 경로를 file_path에 저장\n",
    "# 준비한 이미지 파일의 경로를 이용하여, 이미지 파일을 읽음\n",
    "# cv2.imread(경로): 경로에 해당하는 이미지 파일을 읽어서 변수에 저장\n",
    "img_path = os.getenv('HOME')+'/aiffel/human_segmentation/images/shallowfocus_person.jpg'  \n",
    "img_orig = cv2.imread(img_path) \n",
    "\n",
    "print(img_orig.shape)\n",
    "\n",
    "# cv2.cvtColor(입력 이미지, 색상 변환 코드): 입력 이미지의 색상 채널을 변경\n",
    "# cv2.COLOR_BGR2RGB: 이미지 색상 채널을 변경 (BGR 형식을 RGB 형식으로 변경)\n",
    "# plt.imshow(): 저장된 데이터를 이미지의 형식으로 표시, 입력은 RGB(A) 데이터 혹은 2D 스칼라 데이터\n",
    "# https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\n",
    "# plt.show(): 현재 열려있는 모든 figure를 표시 (여기서 figure는 이미지, 그래프 등)\n",
    "# https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html\n",
    "plt.imshow(cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af62a3",
   "metadata": {},
   "source": [
    "### PixelLib에서 제공해 주는 모델 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac10c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장할 파일 이름을 결정합니다\n",
    "# 1. os.getenv(x)함수는 환경 변수x의 값을 포함하는 문자열 변수를 반환합니다. model_dir 에 \"/aiffel/human_segmentation/models\" 저장\n",
    "# 2. #os.path.join(a, b)는 경로를 병합하여 새 경로 생성 model_file 에 \"/aiffel/aiffel/human_segmentation/models/deeplabv3_xception_tf_dim_ordering_tf_kernels.h5\" 저장\n",
    "# 1\n",
    "model_dir = os.getenv('HOME')+'/aiffel/human_segmentation/models_2' \n",
    "# 2\n",
    "model_file = os.path.join(model_dir, 'deeplabv3_xception_tf_dim_ordering_tf_kernels.h5') \n",
    "\n",
    "# PixelLib가 제공하는 모델의 url입니다\n",
    "model_url = 'https://github.com/ayoolaolafenwa/PixelLib/releases/download/1.1/deeplabv3_xception_tf_dim_ordering_tf_kernels.h5' \n",
    "\n",
    "# 다운로드를 시작합니다\n",
    "urllib.request.urlretrieve(model_url, model_file) # urllib 패키지 내에 있는 request 모듈의 urlretrieve 함수를 이용해서 model_url에 있는 파일을 다운로드 해서 model_file 파일명으로 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c32755",
   "metadata": {},
   "source": [
    "### 우리가 사용할 세그멘테이션 모델을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b47f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = semantic_segmentation() #PixelLib 라이브러리 에서 가져온 클래스를 가져와서 semantic segmentation을 수행하는 클래스 인스턴스를 만듬\n",
    "model.load_pascalvoc_model(model_file) # pascal voc에 대해 훈련된 예외 모델(model_file)을 로드하는 함수를 호출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce98b6b2",
   "metadata": {},
   "source": [
    "### 모델에 이미지 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0b6cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "segvalues, output = model.segmentAsPascalvoc(img_path) # segmentAsPascalvoc()함 수 를 호출 하여 입력된 이미지를 분할, 분할 출력의 배열을 가져옴, 분할 은 pacalvoc 데이터로 학습된 모델을 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141c858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pascalvoc 데이터의 라벨종류\n",
    "LABEL_NAMES = [\n",
    "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n",
    "]\n",
    "len(LABEL_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca1301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#segmentAsPascalvoc() 함수 를 호출하여 입력된 이미지를 분할한 뒤 나온 결과값 중 output을 matplotlib을 이용해 출력\n",
    "plt.imshow(output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d8445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "segvalues # segmentAsPascalvoc() 함수를 호출하여 입력된 이미지를 분할한 뒤 나온 결과값 중 배열값을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08ac1f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#segvalues에 있는 class_ids를 담겨있는 값을 통해 pacalvoc에 담겨있는 라벨을 출력\n",
    "for class_id in segvalues['class_ids']:\n",
    "    print(LABEL_NAMES[class_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f16983",
   "metadata": {},
   "source": [
    "### 컬러맵(어떤 색상으로 나타나 있는지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d2c1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 코드를 이해하지 않아도 좋습니다\n",
    "# PixelLib에서 그대로 가져온 코드입니다\n",
    "# 주목해야 할 것은 생상 코드 결과물이예요!\n",
    "\n",
    "#컬러맵 만들기 \n",
    "colormap = np.zeros((256, 3), dtype = int)\n",
    "ind = np.arange(256, dtype=int)\n",
    "\n",
    "for shift in reversed(range(8)):\n",
    "    for channel in range(3):\n",
    "        colormap[:, channel] |= ((ind >> channel) & 1) << shift\n",
    "    ind >>= 3\n",
    "\n",
    "colormap[:20] #생성한 20개의 컬러맵 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1848bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap[15] #컬러맵 15에 해당하는 배열 출력 (pacalvoc에 LABEL_NAMES 15번째인 사람)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4891cd",
   "metadata": {},
   "source": [
    "### RGB -> BGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2e248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_color = (128,128,192) # 색상순서 변경 - colormap의 배열은 RGB 순이며 output의 배열은 BGR 순서로 채널 배치가 되어 있어서"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cd3ded",
   "metadata": {},
   "source": [
    "### seg_color로만 이루어진 마스크 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad1f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output의 픽셀 별로 색상이 seg_color와 같다면 1(True), 다르다면 0(False)이 됩니다\n",
    "# seg_color 값이 person을 값이 므로 사람이 있는 위치를 제외하고는 gray로 출력\n",
    "# cmap 값을 변경하면 다른 색상으로 확인이 가능함\n",
    "seg_map = np.all(output==seg_color, axis=-1) \n",
    "print(seg_map.shape) \n",
    "plt.imshow(seg_map, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b881df",
   "metadata": {},
   "source": [
    "### 원본이미지와 겹쳐보기(세그멘테이션이 얼마나 잘되었는지 확인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be22da10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본이미지를 img_show에 할당한뒤 이미지 사람이 있는 위치와 배경을 분리해서 표현한 color_mask 를 만든뒤 두 이미지를 합쳐서 출력\n",
    "img_show = img_orig.copy()\n",
    "\n",
    "# True과 False인 값을 각각 255과 0으로 바꿔줍니다\n",
    "img_mask = seg_map.astype(np.uint8) * 255\n",
    "\n",
    "# 255와 0을 적당한 색상으로 바꿔봅니다\n",
    "color_mask = cv2.applyColorMap(img_mask, cv2.COLORMAP_JET)\n",
    "\n",
    "# 원본 이미지와 마스트를 적당히 합쳐봅니다\n",
    "# 0.6과 0.4는 두 이미지를 섞는 비율입니다.\n",
    "img_show = cv2.addWeighted(img_show, 0.6, color_mask, 0.4, 0.0)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img_show, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51415115",
   "metadata": {},
   "source": [
    "### blur 처리(전체 사진을 blur처리 한 후, 배경만 추출)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47d0492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (13,13)은 blurring kernel size를 뜻합니다\n",
    "# 다양하게 바꿔보세요\n",
    "img_orig_blur = cv2.blur(img_orig, (20,20))\n",
    "\n",
    "# plt.imshow(): 저장된 데이터를 이미지의 형식으로 표시한다.\n",
    "# cv2.cvtColor(입력 이미지, 색상 변환 코드): 입력 이미지의 색상 채널을 변경\n",
    "# cv2.COLOR_BGR2RGB: 원본이 BGR 순서로 픽셀을 읽다보니\n",
    "# 이미지 색상 채널을 변경해야함 (BGR 형식을 RGB 형식으로 변경)   \n",
    "plt.imshow(cv2.cvtColor(img_orig_blur, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c5e21b",
   "metadata": {},
   "source": [
    "### 배경 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64496791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.cvtColor(입력 이미지, 색상 변환 코드): 입력 이미지의 색상 채널을 변경\n",
    "# cv2.COLOR_BGR2RGB: 원본이 BGR 순서로 픽셀을 읽다보니\n",
    "# 이미지 색상 채널을 변경해야함 (BGR 형식을 RGB 형식으로 변경) \n",
    "img_mask_color = cv2.cvtColor(img_mask, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "# cv2.bitwise_not(): 이미지가 반전됩니다. 배경이 0 사람이 255 였으나\n",
    "# 연산을 하고 나면 배경은 255 사람은 0입니다.\n",
    "img_bg_mask = cv2.bitwise_not(img_mask_color)\n",
    "\n",
    "# cv2.bitwise_and()을 사용하면 배경만 있는 영상을 얻을 수 있습니다.\n",
    "# 0과 어떤 수를 bitwise_and 연산을 해도 0이 되기 때문에 \n",
    "# 사람이 0인 경우에는 사람이 있던 모든 픽셀이 0이 됩니다. 결국 사람이 사라지고 배경만 남아요!\n",
    "img_bg_blur = cv2.bitwise_and(img_orig_blur, img_bg_mask)\n",
    "plt.imshow(cv2.cvtColor(img_bg_blur, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b269012",
   "metadata": {},
   "source": [
    "### blur처리 된 배경과 원본 이미지의 사람 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f1c4b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.where(조건, 참일때, 거짓일때)\n",
    "# 세그멘테이션 마스크가 255인 부분만 원본 이미지 값을 가지고 오고 \n",
    "# 아닌 영역은 블러된 이미지 값을 사용합니다.\n",
    "img_concat = np.where(img_mask_color==255, img_orig, img_bg_blur)\n",
    "# plt.imshow(): 저장된 데이터를 이미지의 형식으로 표시한다.\n",
    "# cv2.cvtColor(입력 이미지, 색상 변환 코드): 입력 이미지의 색상 채널을 변경\n",
    "# cv2.COLOR_BGR2RGB: 원본이 BGR 순서로 픽셀을 읽다보니 \n",
    "# 이미지 색상 채널을 변경해야함 (BGR 형식을 RGB 형식으로 변경)\n",
    "plt.imshow(cv2.cvtColor(img_concat, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27593a6c",
   "metadata": {},
   "source": [
    "## 1-2. 고먐미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b10a848",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.getenv('HOME')+'/aiffel/human_segmentation/images/cat.jpg'  \n",
    "img_orig = cv2.imread(img_path) \n",
    "print(img_orig.shape)\n",
    "plt.imshow(cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6530ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.getenv('HOME')+'/aiffel/human_segmentation/models_2' \n",
    "\n",
    "model_file = os.path.join(model_dir, 'deeplabv3_xception_tf_dim_ordering_tf_kernels.h5') \n",
    "\n",
    "# PixelLib가 제공하는 모델의 url\n",
    "model_url = 'https://github.com/ayoolaolafenwa/PixelLib/releases/download/1.1/deeplabv3_xception_tf_dim_ordering_tf_kernels.h5' \n",
    "\n",
    "# 다운로드 시작\n",
    "urllib.request.urlretrieve(model_url, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85065e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = semantic_segmentation() \n",
    "model.load_pascalvoc_model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd4a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "segvalues, output = model.segmentAsPascalvoc(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6950da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_NAMES = [\n",
    "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n",
    "]\n",
    "len(LABEL_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9c500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0836513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "segvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf8e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_id in segvalues['class_ids']:\n",
    "    print(LABEL_NAMES[class_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5084c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = np.zeros((256, 3), dtype = int)\n",
    "ind = np.arange(256, dtype=int)\n",
    "\n",
    "for shift in reversed(range(8)):\n",
    "    for channel in range(3):\n",
    "        colormap[:, channel] |= ((ind >> channel) & 1) << shift\n",
    "    ind >>= 3\n",
    "\n",
    "colormap[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e5905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap[8]    #고양이 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf82053",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_color = (0,0,64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0ae2ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seg_map = np.all(output==seg_color, axis=-1) \n",
    "print(seg_map.shape) \n",
    "plt.imshow(seg_map, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a2b65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_show = img_orig.copy()\n",
    "\n",
    "img_mask = seg_map.astype(np.uint8) * 255\n",
    "\n",
    "color_mask = cv2.applyColorMap(img_mask, cv2.COLORMAP_JET)\n",
    "\n",
    "img_show = cv2.addWeighted(img_show, 0.6, color_mask, 0.4, 0.0)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img_show, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b49ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_orig_blur = cv2.blur(img_orig, (20,20)) \n",
    "plt.imshow(cv2.cvtColor(img_orig_blur, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66db6fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mask_color = cv2.cvtColor(img_mask, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "img_bg_mask = cv2.bitwise_not(img_mask_color)\n",
    "\n",
    "img_bg_blur = cv2.bitwise_and(img_orig_blur, img_bg_mask)\n",
    "plt.imshow(cv2.cvtColor(img_bg_blur, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a1318",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_concat = np.where(img_mask_color==255, img_orig, img_bg_blur)\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.imshow(cv2.cvtColor(img_concat, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2049696",
   "metadata": {},
   "source": [
    "## 1-3. 크로마키 합성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c31ce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bg_img_path = os.getenv('HOME')+'/aiffel/human_segmentation/images/sky.jpg'  # 본인이 사용할 사진의 경로\n",
    "bg_img_orig = cv2.imread(bg_img_path)\n",
    "print(bg_img_orig.shape)\n",
    "\n",
    "bg_img_rgb = cv2.cvtColor(bg_img_orig, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(bg_img_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24124248",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_img_path = os.getenv('HOME')+'/aiffel/human_segmentation/images/cat.jpg'  # 본인이 사용할 사진의 경로\n",
    "cat_img_orig = cv2.imread(cat_img_path)\n",
    "print(cat_img_orig.shape)\n",
    "cat_img_rgb = cv2.cvtColor(cat_img_orig, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(cat_img_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eec760",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (cat_img_orig.shape[1], cat_img_orig.shape[0])\n",
    "bg_resized = cv2.resize(bg_img_orig, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03daa12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mask_color = cv2.cvtColor(img_mask, cv2.COLOR_GRAY2BGR)\n",
    "img_concat3 = np.where(img_mask_color==255, cat_img_orig, bg_resized)\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.imshow(cv2.cvtColor(img_concat3, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a7fe06",
   "metadata": {},
   "source": [
    "## 2. 사진 문제점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46838ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.getenv('HOME')+'/aiffel/human_segmentation/tagged.png'  \n",
    "img_orig = cv2.imread(img_path) \n",
    "plt.figure(figsize = (15,15))\n",
    "plt.imshow(cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8e8458",
   "metadata": {},
   "source": [
    "- 인물 영역의 경계가 뚜렷하지 않아서 머리와 왼쪽 옷 부분이 블러처리 되었다.  \n",
    "- 앞을 보고 있는 사진이 아닌, 사람의 뒷모습이어서 인물을 더 정확히 인식하지 못했을 수도 있을 것 같다.\n",
    "- 해상도가 낮다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4554671",
   "metadata": {},
   "source": [
    "## 3. 솔루션"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d9b6a8",
   "metadata": {},
   "source": [
    "- 우리가 해당 과제에서 사용한 DeepLab V3 encoder를 통과해서 나온 feature map은 원본 사진의 해상도보다 16배가 작다.(Output Stride: 16). 같은 모델의 decoder에서는 encoder feature map을 단순히 16배 bilinear up-sample한다. \n",
    "- 하지만 이러면 segmentation 해상도가 너무 떨어지는 단점이 있다. \n",
    "- 따라서 DeepLab V3+에서는 이전의 FCN이나 UNET에서 사용하였던 것처럼 Skip architecture를 도입하여 Encoder와 Decoder를 연결시켜준다.  \n",
    "https://wikidocs.net/143446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda4323f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
